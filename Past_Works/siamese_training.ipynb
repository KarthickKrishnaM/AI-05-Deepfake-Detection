{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9X0pifUXUbwk"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import dlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BkACd5MwYZwu"
      },
      "outputs": [],
      "source": [
        "!pip install opendatasets --upgrade --quiet\n",
        "import opendatasets as od"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vb9it6nGYamU"
      },
      "outputs": [],
      "source": [
        "dataset_url=\"https://www.kaggle.com/datasets/sorokin/faceforensics\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0qettRZPmD1v"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "sns.set(style=\"whitegrid\")\n",
        "import os\n",
        "import glob as gb\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "import time\n",
        "import random\n",
        "import torchvision.transforms as t\n",
        "import torchvision.models as models\n",
        "import albumentations as A"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J3ha9-aJYch_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb026c43-da96-4d96-99ed-029c9eb917f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping, found downloaded files in \"./faceforensics\" (use force=True to force download)\n"
          ]
        }
      ],
      "source": [
        "od.download(dataset_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S8VvRNH9UgwS"
      },
      "outputs": [],
      "source": [
        "detector = dlib.get_frontal_face_detector()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DTPBxyPAaXUx"
      },
      "outputs": [],
      "source": [
        "from google.colab.patches import cv2_imshow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cVplRIuTE-0U"
      },
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ved-iRSIXLJC"
      },
      "outputs": [],
      "source": [
        "fake = '/content/faceforensics/manipulated_sequences/Deepfakes/c23/videos/'\n",
        "original = '/content/faceforensics/original_sequences/youtube/c23/videos/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gD25pnanXTVd"
      },
      "outputs": [],
      "source": [
        "os.mkdir(\"images\")\n",
        "\n",
        "os.mkdir(\"images/real\")\n",
        "os.mkdir(\"images/fake\")\n",
        "\n",
        "os.mkdir(\"images/fake/train\")\n",
        "os.mkdir(\"images/fake/test\")\n",
        "os.mkdir(\"images/fake/validation\")\n",
        "\n",
        "os.mkdir(\"images/real/train\")\n",
        "os.mkdir(\"images/real/test\")\n",
        "os.mkdir(\"images/real/validation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "745v518NGHHv"
      },
      "outputs": [],
      "source": [
        "def save(img,file_path, folder_name, name, bbox, width=180,height=227):\n",
        "    x, y, w, h = bbox\n",
        "    imgCrop = img[y:h, x: w]\n",
        "    if imgCrop.any():\n",
        "      imgCrop = cv2.resize(imgCrop, (width, height)) #we need this line to reshape the images\n",
        "      cv2.imwrite(file_path+folder_name+name+\".jpg\", imgCrop)\n",
        "\n",
        "def faces(vid_path,file_path):\n",
        "    i=0\n",
        "    vids = 0\n",
        "    for file in os.listdir(vid_path):\n",
        "      path=os.path.join(vid_path, file)\n",
        "      cap = cv2.VideoCapture(path)\n",
        "      _, frame = cap.read()\n",
        "      gray =cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
        "      faces = detector(gray)\n",
        "      if vids<720:\n",
        "        folder_name = \"train/\"\n",
        "      elif vids<860:\n",
        "        folder_name = \"test/\"\n",
        "      else:\n",
        "        folder_name = \"validation/\"\n",
        "    # detect the face\n",
        "      if len(faces):\n",
        "        for counter,face in enumerate(faces):\n",
        "            x1, y1 = face.left(), face.top()\n",
        "            x2, y2 = face.right(), face.bottom()\n",
        "            save(frame,file_path,folder_name,str(i),(x1,y1,x2,y2))\n",
        "            i=i+1\n",
        "      vids+=1\n",
        "    cap.release()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FUcRPZKNmhvR"
      },
      "outputs": [],
      "source": [
        "faces(fake,'/content/images/fake/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WlFz0frPc_kD"
      },
      "outputs": [],
      "source": [
        "faces(original,'/content/images/real/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sf1QIB0Gnhra"
      },
      "outputs": [],
      "source": [
        "import torch.utils.data as data\n",
        "from torchvision import datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xIiymcY00D8K"
      },
      "outputs": [],
      "source": [
        "x_real = []\n",
        "y_real = []\n",
        "x_fake=[]\n",
        "y_fake=[]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "augmentation = A.Compose(\n",
        "    [\n",
        "        A.Downscale(p=0.5),\n",
        "        A.HorizontalFlip(p=0.5),\n",
        "        A.RandomBrightnessContrast(p=0.5),\n",
        "        A.HueSaturationValue(p=0.5),\n",
        "        A.GaussNoise(p=0.5),\n",
        "        A.JpegCompression(p=0.5)\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "fXTX15OMJZgM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e70e2be9-fc10-483d-8b9f-1c4065803eb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/albumentations/augmentations/transforms.py:778: FutureWarning: JpegCompression has been deprecated. Please use ImageCompression\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xNFgF4Oaf26F"
      },
      "outputs": [],
      "source": [
        "for folder in  os.listdir(\"/content/images/\") :\n",
        "    files = gb.glob(pathname= str( \"/content/images/\" + folder + \"/train\" + '/*.jpg'))\n",
        "    for file in files:\n",
        "      image = cv2.imread(file)\n",
        "      image = np.array(cv2.resize(image,(224,224)))\n",
        "      aug = augmentation(image=image)\n",
        "      image = aug[\"image\"]\n",
        "      image = (torch.tensor(image).permute(2,0,1) )/255\n",
        "      if folder==\"real\":\n",
        "        y_real.append(0)\n",
        "        x_real.append(image)\n",
        "      else:\n",
        "        y_fake.append(1)\n",
        "        x_fake.append(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ETkWzd0EoWTB"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class customDataset(Dataset):\n",
        "    def __init__(self, positive_data,negative_data, positive_labels,negative_labels, transform = t.Resize(224)):\n",
        "        self.positive_data = positive_data\n",
        "        self.negative_data=negative_data\n",
        "        self.positive_labels = torch.LongTensor(positive_labels)\n",
        "        self.negative_labels=torch.LongTensor(negative_labels)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        anchor_img = self.positive_data[index]\n",
        "        positive_img=random.choice(self.positive_data)\n",
        "        #while positive_img==anchor_img:\n",
        "          #positive_img=random.choice(self.positive_data)\n",
        "        negative_img=random.choice(self.negative_data)\n",
        "        anchor_label = self.positive_labels[index]\n",
        "        if self.transform:\n",
        "          anchor_img = self.transform(anchor_img)\n",
        "          positive_img = self.transform(positive_img)\n",
        "          negative_img = self.transform(negative_img)\n",
        "          return anchor_img,positive_img,negative_img,anchor_label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.positive_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "If__fHyktzZV"
      },
      "outputs": [],
      "source": [
        "train_realanchor_set=customDataset(x_real,x_fake,y_real,y_fake)\n",
        "train_fakeanchor_set=customDataset(x_fake,x_real,y_fake,y_real)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rsE7H6bdvrLb"
      },
      "outputs": [],
      "source": [
        "train_realanchor_loader=DataLoader(train_realanchor_set, batch_size=6, shuffle=True)\n",
        "train_fakeanchor_loader=DataLoader(train_fakeanchor_set, batch_size=6, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "b4 = models.efficientnet_b4(pretrained=False)\n",
        "\n",
        "class efficientnet_b4_att(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(efficientnet_b4_att, self).__init__()\n",
        "        self.first = b4.features[0:4]\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Conv2d(56,1,1),\n",
        "            nn.Sigmoid(),\n",
        "            )\n",
        "        self.last = b4.features[4:]\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.last(self.first(x) * self.attention(self.first(x)))\n",
        "        return x"
      ],
      "metadata": {
        "id": "bCa2BVzOYEFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EfficientViT(nn.Module):\n",
        "  def __init__(self, config, channels=512, selected_efficient_net = 0):\n",
        "        super().__init__()\n",
        ""
      ],
      "metadata": {
        "id": "w4g0yjKlqKIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9mvFvSLe_bCU"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = efficientnet_b4_att().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WzObclbeEJRT"
      },
      "outputs": [],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RtRhkP4g_u4j"
      },
      "outputs": [],
      "source": [
        "class TripletLoss(nn.Module):\n",
        "    def __init__(self, margin=2.5):\n",
        "        super(TripletLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "\n",
        "    def calc_euclidean(self, x1, x2):\n",
        "        #print('x1:',x1.size())\n",
        "        return (x1 - x2).pow(2).sum(1)\n",
        "\n",
        "    def forward(self, anchor: torch.Tensor, positive: torch.Tensor, negative: torch.Tensor) -> torch.Tensor:\n",
        "        distance_positive = self.calc_euclidean(anchor, positive)\n",
        "        distance_negative = self.calc_euclidean(anchor, negative)\n",
        "        #print('p:',distance_positive)\n",
        "        #print('n',distance_negative)\n",
        "        losses = torch.relu(distance_positive - distance_negative + self.margin)\n",
        "       # print('loss:',losses.size())\n",
        "       # print('anchor:',anchor.size())\n",
        "        return losses.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_ahUIoYYxOT"
      },
      "outputs": [],
      "source": [
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JqSYRrnzfSHD"
      },
      "outputs": [],
      "source": [
        "class LosslessTripletLoss(nn.Module):\n",
        "    def __init__(self,beta,epsilon,n):\n",
        "        super(LosslessTripletLoss, self).__init__()\n",
        "        self.beta=beta\n",
        "        self.epsilon=epsilon\n",
        "        self.dim=n\n",
        "\n",
        "    def positive_dist(self, x1, x2):\n",
        "        print('root:',(x1 - x2).pow(2))\n",
        "        return (-math.log((-((x1 - x2).pow(2)))/self.beta)+1+self.epsilon).sum(1)\n",
        "\n",
        "    def negative_dist(self,x1,x2):\n",
        "        return (-math.log((-(self.dim-((x1 - x2).pow(2))))/self.beta)+1+self.epsilon).sum(1)\n",
        "\n",
        "    def forward(self, anchor: torch.Tensor, positive: torch.Tensor, negative: torch.Tensor) -> torch.Tensor:\n",
        "        distance_positive = self.positive_dist(anchor, positive)\n",
        "        distance_negative = self.negative_dist(anchor, negative)\n",
        "        print('p:',distance_positive)\n",
        "        print('n',distance_negative)\n",
        "        losses = distance_positive + distance_negative\n",
        "       # print('loss:',losses.size())\n",
        "       # print('anchor:',anchor.size())\n",
        "        return losses.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aWhIa3iSDyQO"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-9VAsWhGdZT"
      },
      "outputs": [],
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = torch.jit.script(LosslessTripletLoss(1792,1e-8,1792))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7n1z7NnREHWQ"
      },
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "epochs=50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rAh1crzM_ISw"
      },
      "outputs": [],
      "source": [
        "model.train()\n",
        "outputs=[]\n",
        "for epoch in tqdm(range(epochs), desc=\"Epochs\"):\n",
        "    running_loss1 = []\n",
        "    running_loss2 = []\n",
        "    for step1, (anchor_img1, positive_img1, negative_img1, anchor_label1) in enumerate(tqdm(train_realanchor_loader, desc=\"Training\", leave=False)):\n",
        "        anchor_img1 = anchor_img1.to(device)\n",
        "        positive_img1 = positive_img1.to(device)\n",
        "        negative_img1 = negative_img1.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        anchor_out1 = model.forward(anchor_img1)\n",
        "        positive_out1 = model.forward(positive_img1)\n",
        "        negative_out1= model.forward(negative_img1)\n",
        "\n",
        "        loss = criterion(anchor_out1, positive_out1, negative_out1)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss1.append(loss.cpu().detach().numpy())\n",
        "    print(\"Epoch: {}/{} - Loss1: {:.4f}\".format(epoch+1, epochs, np.mean(running_loss1)))\n",
        "\n",
        "\n",
        "    for step2, (anchor_img2, positive_img2, negative_img2, anchor_label2) in enumerate(tqdm(train_fakeanchor_loader, desc=\"Training\", leave=False)):\n",
        "        anchor_img2 = anchor_img2.to(device)\n",
        "        positive_img2 = positive_img2.to(device)\n",
        "        negative_img2 = negative_img2.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        anchor_out2 = model.forward(anchor_img2)\n",
        "        positive_out2 = model.forward(positive_img2)\n",
        "        negative_out2= model.forward(negative_img2)\n",
        "\n",
        "        loss = criterion(anchor_out2, positive_out2, negative_out2)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss2.append(loss.cpu().detach().numpy())\n",
        "    print(\"Epoch: {}/{} - Loss2: {:.4f}\".format(epoch+1, epochs, np.mean(running_loss2)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ukIRmcJGINtj"
      },
      "outputs": [],
      "source": [
        "for param in model.parameters():\n",
        "    param.requires_grad = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZdABEXxum5X0"
      },
      "outputs": [],
      "source": [
        "x_train=[]\n",
        "y_train=[]\n",
        "for i in range(0,len(x_real)):\n",
        "  x_train.append(x_real[i])\n",
        "  y_train.append(y_real[i])\n",
        "for i in range(0,len(x_fake)):\n",
        "  x_train.append(x_fake[i])\n",
        "  y_train.append(y_fake[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UxselAo2n-yL"
      },
      "outputs": [],
      "source": [
        "# x_test=[]\n",
        "# y_test=[]\n",
        "# for i in range(0,len(x_real_val)):\n",
        "#   x_test.append(x_real_val[i])\n",
        "#   y_test.append(y_real_val[i])\n",
        "# for i in range(0,len(x_fake)):\n",
        "#   x_test.append(x_fake_val[i])\n",
        "#   y_test.append(y_fake_val[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eb49xZJEB2z-"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class customDataset(Dataset):\n",
        "    def __init__(self, data, labels, transform = None):\n",
        "        self.data = data\n",
        "        self.labels = torch.FloatTensor(labels)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        x = self.data[index]\n",
        "        if len(self.labels):\n",
        "          y = self.labels[index]\n",
        "        else:\n",
        "          y = None\n",
        "        if self.transform:\n",
        "            x = self.transform(x)\n",
        "        return x, y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C5wazTcECtzM"
      },
      "outputs": [],
      "source": [
        "train_finetuning = customDataset(x_train,y_train)\n",
        "# test_dataset=customDataset(x_test,y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5GmsN1GK905O"
      },
      "outputs": [],
      "source": [
        "train_finetuning_loader=DataLoader(train_finetuning, batch_size=64, shuffle=True)\n",
        "# test_loader=DataLoader(test_dataset,batch_size=64,shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vHkKaLY2FppW"
      },
      "outputs": [],
      "source": [
        "class finetuning_layer(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(finetuning_layer, self).__init__()\n",
        "    self.classifier=nn.Linear(1792,1)\n",
        "  def forward(self,x):\n",
        "    x = model.forward(x).squeeze()\n",
        "    return self.classifier(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t88QpRXIDw88"
      },
      "outputs": [],
      "source": [
        "final_layer_model=finetuning_layer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qkGn92PFEZo-"
      },
      "outputs": [],
      "source": [
        "final_layer_model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PrgGm5BVwQ7t"
      },
      "outputs": [],
      "source": [
        "epochs = 50\n",
        "lr = 0.001\n",
        "optimizer = optim.Adam\n",
        "criterion = torch.nn.functional.binary_cross_entropy_with_logits\n",
        "\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs\n",
        "\n",
        "def accuracy(y_pred, y):\n",
        "   # predicted = torch.max(y_pred.data)\n",
        "    predicted=torch.round(y_pred)\n",
        "    total = y.size(0)\n",
        "    correct = (predicted == y).sum().item()\n",
        "    return correct/total\n",
        "\n",
        "def train(model, dataset, opt_fn, criterion,epoch,learning_rate):\n",
        "\n",
        "    optimizer=opt_fn(model.parameters(),learning_rate)\n",
        "    final_layer_model.train()\n",
        "    train_loss=[]\n",
        "    train_acc=[]\n",
        "    for batch_idx,(data,target) in enumerate(dataset):\n",
        "\n",
        "        data, target = data.cuda(), target.cuda()\n",
        "        optimizer.zero_grad()\n",
        "        output=final_layer_model(data)\n",
        "        output=torch.squeeze(output,1)\n",
        "        #loss=criterion(output,target)\n",
        "        logloss=torch.nn.functional.cross_entropy(output,targets)\n",
        "        logloss.backward()\n",
        "        optimizer.step()\n",
        "        acc = accuracy(output, target)\n",
        "        train_acc.append(acc)\n",
        "        train_loss.append(logloss.item())\n",
        "        print('\\repoch:{}({:.0f}%)\\tloss:{:.3f}\\ttrain_accuracy:{:.2f}%'.format(epoch+1,100*batch_idx/len(dataset),\n",
        "        np.mean(train_loss),100*np.mean(train_acc)),end='')\n",
        "\n",
        "def eval(model, dataset, criterion):\n",
        "\n",
        "    model.eval()\n",
        "    val_acc=[]\n",
        "    for batch_idx,(data,target) in enumerate(dataset):\n",
        "        output=model(data)\n",
        "        acc = accuracy(output, target)\n",
        "        val_acc.append(acc)\n",
        "    print('val_accuracy:{:.2f}%'.format(100*np.mean(val_acc)))\n",
        "    return np.mean(val_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iSWJtwCmIJT_"
      },
      "outputs": [],
      "source": [
        "for epoch in range(epochs):\n",
        "      start_time = time.monotonic()\n",
        "      train(final_layer_model, train_finetuning_loader, optimizer, criterion,epoch,lr)\n",
        "      # eval_accuracy = eval(model, test_loader, criterion)\n",
        "      end_time = time.monotonic()\n",
        "      epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "      print(\"TIME TAKEN FOR THE EPOCH: {} mins and {} seconds\\n\".format(epoch_mins, epoch_secs))\n",
        "print(\"OVERALL TRAINING COMPLETE\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n1LhhltCVily"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}